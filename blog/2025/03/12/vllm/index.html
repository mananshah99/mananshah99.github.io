<html>
<head>
    <title>Batteries-included performant language model serving with vLLM</title>
    <meta charset='UTF-8'>
    <meta content='width=device-width, initial-scale=1' name='viewport'/>

    <meta name='description' content='Manan Shah'>
    <meta name='keywords' content='machine learning, systems'>
    <meta name='author' content='Manan Shah'>

    <link href='/css/callout.css' rel='stylesheet'/>
    <link href='/css/blog.css' rel='stylesheet'/>
    <link href='/css/trac.css' rel='stylesheet'/>
    <link href='/css/markdown.css' rel='stylesheet'/>

    <script type='text/x-mathjax-config'>
            
MathJax.Hub.Config({
  jax: ['input/TeX', 'output/HTML-CSS'],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    extensions: ['color.js']
  },
  messageStyle: 'none',
  'HTML-CSS': { preferredFont: 'TeX', availableFonts: ['STIX','TeX'] }
});

</script>

<!-- this is necessary to get the mathjax config working! -->
<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  }
};
</script>

<script type='text/javascript' src='https://cdn.jsdelivr.net/npm/mathjax@3.0.0/es5/tex-mml-chtml.js'></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">



    <!--- <script src="/assets/katex.min.js"></script> -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.js" integrity="sha512-LQNxIMR5rXv7o+b1l8+N1EZMfhG7iFZ9HhnbJkTp4zjNr5Wvst75AqUeFDxeRUa7l5vEDyUiAip//r+EFLLCyA==" crossorigin="anonymous"></script>
    <script src="/assets/pseudocode.min.js" type="text/javascript"></script>
    <link rel="stylesheet" href="/assets/pseudocode.min.css" type="text/css">

</head>
<body>
<div class='content'>
    <div class='nav'>
    <ul class='wrap'>
        <li><a href='/'>Home</a></li>
        <li><a href='/blog'>Blog</a></li>
        <li><a href='/feed.xml'>RSS</a></li>
    </ul>
</div>
    <div class='front-matter'>
        <div class='wrap'>
            <h1>Batteries-included performant language model serving with vLLM</h1>
            <h4>We walk through the architecture, dataflow, and some design decisions of the vLLM library.</h4>
            <div class='bylines'>
                <div class='byline'>
                    <h3>Published&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Updated&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Categories</h3>
                    <p>2025-03-15 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2025-03-15&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;machine learning, systems</p>
                </div>
            </div>
            <div class='clear'></div>
        </div>
    </div>
    <div class='wrap article'>
        <h2 id="introduction">Introduction</h2>

<p>With the advent of powerful open-weights large language models (<em>e.g.</em> the Meta
AI Llama series, Mistral (8x)7B, the Deepseek series, and the Qwen series),
much attention has been placed on the infrastructure and optimization of
local/distributed inference and finetuning. vLLM, developed around the
introduction of <a href="https://arxiv.org/abs/2309.06180">paged attention</a><sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>, is one
such library. The library boasts an impressive following, with surprisingly
thorough documentation, a vast set of features (<em>e.g.</em> quantization, LoRA
adapters, tool calling, structured outputs, specultaive decoding, prefix
caching, and more), and integrations with extensions that support pre- and
post-training. It’s therefore a natural library to peek under the hood of:
that’s what we’ll attempt to do in this post.</p>

<p>Note that many components of vLLM deserve blog posts in their own right;
subsystems like the vLLM implementation of flash attention, speculative
decoding, paged attention, and others may be covered in future articles.
In this post, we’ll build intuition for the library’s architecture,
dataflow, and core components. Our goal is to develop an understanding of
the library’s design, and the way modules interact in the serving flow.<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup></p>

<div class="figure">
    <img src="/assets/vllm.svg" style="width: 210%; display: block; margin-left: -230px; margin-right: -40px;" />
    <div class="caption">
        <span class="caption-label">Figure 1.</span> A map of the important classes
        that play a role in vLLM serving; the primary components are the LLM
        engine, scheduler, executor, and model runner. Grey boxes are
        implementations, white boxes are abstract classes, and brown boxes are
        dataclasses. The model runner, which performs the (sharded) model
        forward pass, is colored green for emphasis. The figure is quite large;
        please open it in a new tab if text is too small to read.
    </div>
</div>

<h2 id="entrypoints">Entrypoint(s)</h2>

<p>vLLM supports multiple entrypoints against which model inference can be
performed: examples include an API server, a command line interface, and Python
classes (<code class="language-plaintext highlighter-rouge">LLM</code> for offline/batch inference, and <code class="language-plaintext highlighter-rouge">AsyncLLMEngine</code> for
real-time inference). Here, we’ll focus on a non-pipeline-parallel scenario
utilizing the Python <code class="language-plaintext highlighter-rouge">LLM</code> entrypoint that implements offline inference (in
hardware, one can imagine this as a single-node, (possibly) multi-GPU setting).
A full understanding of this pipeline will translate well to other, more
complicated scenarios.</p>

<h3 id="the-llm-python-class-entrypoint">The LLM Python Class Entrypoint</h3>

<p>The <code class="language-plaintext highlighter-rouge">LLM</code> class is really a user-friendly interface for the <code class="language-plaintext highlighter-rouge">LLMEngine</code>, which
is the workhorse that powers the inference pipeline. The class provides the
following important API signatures, among others:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># NOTE `generate`, `encode`, `embed`, `classify` all folllow similar signatures.
# Their difference is in the model architecture (whether the model is "pooled"
# or not):
</span><span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
  <span class="bp">self</span><span class="p">,</span>
  <span class="n">prompts</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">PromptType</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">PromptType</span><span class="p">]],</span>
           <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
  <span class="n">sampling_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">SamplingParams</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">SamplingParams</span><span class="p">]]]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
  <span class="n">lora_request</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="n">LoRARequest</span><span class="p">],</span> <span class="n">LoRARequest</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
  <span class="n">prompt_adapter_request</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PromptAdapterRequest</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
  <span class="n">guided_options_request</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">LLMGuidedOptions</span><span class="p">,</span> <span class="n">GuidedDecodingRequest</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
  <span class="n">priority</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">RequestOutput</span><span class="p">]:</span>

<span class="k">def</span> <span class="nf">beam_search</span><span class="p">(</span>
  <span class="bp">self</span><span class="p">,</span>
  <span class="n">prompts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">TokensPrompt</span><span class="p">,</span> <span class="n">TextPrompt</span><span class="p">]],</span>
  <span class="n">params</span><span class="p">:</span> <span class="n">BeamSearchParams</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">BeamSearchOutput</span><span class="p">]:</span>
</code></pre></div></div>

<p>Inputs are cast to their appropriate types and passed to the workhorse, <code class="language-plaintext highlighter-rouge">LLMEngine</code>,
which we’ll explore next.</p>

<h3 id="the-llm-engine">The LLM Engine</h3>

<p>The LLM engine manages the end-to-end execution of a token generation pipeline,
including a tokenizer, a (possibly distributed) language model, a key-value
cache for intermediate model states, and postprocessing. There exist two
flavors of the LLM engine; <code class="language-plaintext highlighter-rouge">LLMEngine</code> (for offline synchronous execution) and
<code class="language-plaintext highlighter-rouge">AsyncLLMEngine</code> (for online asynchronous execution). The implementations are
somewhat similar; while the former performs all steps of the pipeline in a
synchronous manner, the latter executes as many steps on separate executor
threads as possible, to free the base event loop and support multiple
concurrent generation calls.</p>

<p>While some components of the token generation pipeline are embarrassingly
parallel (<em>e.g.</em> tokenization), other components are more tricky: for example,
natural questions emerge about the batching mechanics of multiple sequences
with <code class="language-plaintext highlighter-rouge">generate</code> requests scheduled close to one another (the answer: continuous
batching), KV cache management when multiple generation calls oversubscribe GPU
memory (the answer: paged attention), and KV cache reuse when multiple requests
share the same prefix (the answer: prefix caching). And these questions become
even more involved when considering their interactions with performance
improvements using techniques such as speculative decoding.</p>

<p>To support modular development and clarify responsibilities of individual
components, the vLLM developers segmented the stack into a few core
modules: the scheduler, executor, worker, and model runner.</p>

<h2 id="scheduler">Scheduler</h2>

<p>A scheduler is defined per vLLM virtual engine<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>: in synchronous mode, there
only exists one virtual engine, while asynchronous mode supports proper
pipelining of requests and therefore supports multiple virtual engines. The job
of the scheduler is to make a <em>scheduling decision</em> for the engine containing
the batch of sequence groups to perform a forward pass on, along with
associated memory management decisions.</p>

<p>Each scheduled sequence group (a group of sequences generated from the same
prompt) is represented by a <code class="language-plaintext highlighter-rouge">SequenceGroupMetadata</code> object, containing its
request ID, metadata about the request, the tokenizer representation of the
input, sampling parameters, and other auxiliary information. The dataclass is
replicated below, with some irrelevant information omitted:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SequenceGroupMetadata</span><span class="p">(</span><span class="n">msgspec</span><span class="p">.</span><span class="n">Struct</span><span class="p">):</span>
  <span class="n">request_id</span><span class="p">:</span> <span class="nb">str</span>
  <span class="n">is_prompt</span><span class="p">:</span> <span class="nb">bool</span>
  <span class="n">seq_data</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">SequenceData</span><span class="p">]</span>
  <span class="n">sampling_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SamplingParams</span><span class="p">]</span>
  <span class="c1"># The block tables: sequence id =&gt; list of physical block numbers:
</span>  <span class="n">block_tables</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span>
  <span class="n">do_sample</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span>
  <span class="n">pooling_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PoolingParams</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>
  <span class="n">lora_request</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LoRARequest</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>
  <span class="c1"># Block numbers that are already computed, for use in prefix caching:
</span>  <span class="n">computed_block_nums</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">None</span>
  <span class="n">state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SequenceGroupState</span><span class="p">]</span> <span class="o">=</span> <span class="n">msgspec</span><span class="p">.</span><span class="n">field</span><span class="p">(</span>
      <span class="n">default_factory</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="n">SequenceGroupState</span><span class="p">())</span>
  
  <span class="c1"># Multimodal information:
</span>  <span class="n">token_type_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">None</span>
  <span class="n">multi_modal_data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>
  <span class="n">multi_modal_placeholders</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MultiModalPlaceholderDict</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>
  <span class="n">token_chunk_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>
</code></pre></div></div>

<p>The memory management information is represented by the <code class="language-plaintext highlighter-rouge">SchedulerOutputs</code>
dataclass, replicated below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">SchedulerOutputs</span><span class="p">:</span>
  <span class="c1"># Scheduled sequence groups.
</span>  <span class="n">scheduled_seq_groups</span><span class="p">:</span> <span class="n">GenericSequence</span><span class="p">[</span><span class="n">ScheduledSequenceGroup</span><span class="p">]</span>
  <span class="c1"># Number of prefill groups scheduled.
</span>  <span class="n">num_prefill_groups</span><span class="p">:</span> <span class="nb">int</span>
  <span class="c1"># Total number of batched tokens.
</span>  <span class="n">num_batched_tokens</span><span class="p">:</span> <span class="nb">int</span>
  <span class="c1"># Blocks to swap in. List of CPU -&gt; GPU block number.
</span>  <span class="n">blocks_to_swap_in</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span>
  <span class="c1"># Blocks to swap out. List of GPU -&gt; CPU block number.
</span>  <span class="n">blocks_to_swap_out</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span>
  <span class="c1"># Blocks to copy. Source to dest block.
</span>  <span class="n">blocks_to_copy</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span>
  <span class="c1"># Sequence groups that are going to be ignored.
</span>  <span class="n">ignored_seq_groups</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">SequenceGroup</span><span class="p">]</span>
  <span class="c1"># The number of slots for lookahead decoding.
</span>  <span class="n">num_lookahead_slots</span><span class="p">:</span> <span class="nb">int</span>
  <span class="c1"># The number of requests in the running queue
</span>  <span class="n">running_queue_size</span><span class="p">:</span> <span class="nb">int</span>
  <span class="n">preempted</span><span class="p">:</span> <span class="nb">int</span>
</code></pre></div></div>

<p>At LLM engine initialization time, schedulers are initialized with empty queues
for sequence groups in the waiting, running, and swapped states. When the
engine receives a request via the <code class="language-plaintext highlighter-rouge">add_request</code> method (one request per prompt
in batch mode), it performs tokenization and adds the resulting “processed”
request to the scheduler’s waiting pool<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>, to be scheduled at the next invocation
of <code class="language-plaintext highlighter-rouge">LLMEngine.step</code>.</p>

<p>When <code class="language-plaintext highlighter-rouge">step</code> is called, the scheduler is first invoked via its <code class="language-plaintext highlighter-rouge">schedule</code> method,
which handles the core business logic for request selection and memory allocation
(ultimately returned as a <code class="language-plaintext highlighter-rouge">List[SequenceGroupMetadata]</code> and <code class="language-plaintext highlighter-rouge">SchedulerOutputs</code>
to the engine). The default scheduler is memory-aware (via its block space
manager) and optimizes for throughput, always preferring to batch as many
prefills as possible and delay decodes as necessary<sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">5</a></sup><sup id="fnref:6"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">6</a></sup>. For more details,
the interested reader is encouraged to examine the <code class="language-plaintext highlighter-rouge">_schedule_default</code> method
in the <code class="language-plaintext highlighter-rouge">Scheduler</code> class.</p>

<p>When the scheduler completes its job, the LLM engine (within <code class="language-plaintext highlighter-rouge">step</code>) receives
the aforementioned scheduled sequence groups and scheduler outputs, which it
consolidates into an <code class="language-plaintext highlighter-rouge">ExecuteModelRequest</code> to be passed to the model executor
for forwarding (in the asynchronous engine, this occurs in a separate thread).</p>

<h2 id="executors-and-workers">Executors and Workers</h2>

<p>A model executor is called from the LLM engine as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">self</span><span class="p">.</span><span class="n">model_executor</span><span class="p">:</span> <span class="n">ExecutorBase</span> <span class="o">=</span> <span class="p">...</span>   <span class="c1"># Initialized upstream
</span><span class="n">execute_model_req</span> <span class="o">=</span> <span class="p">...</span>   <span class="c1"># The request consolidated from the scheduler
</span><span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model_executor</span><span class="p">.</span><span class="n">execute_model</span><span class="p">(</span><span class="n">execute_model_req</span><span class="o">=</span><span class="n">execute_model_req</span><span class="p">)</span>
</code></pre></div></div>

<p>which is implemented in the base executor as a collective remote procedure call
that runs <code class="language-plaintext highlighter-rouge">"execute_model"</code> on all of the executor’s workers and collects the
response:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">execute_model</span><span class="p">(</span>
  <span class="bp">self</span><span class="p">,</span> <span class="n">execute_model_req</span><span class="p">:</span> <span class="n">ExecuteModelRequest</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">SamplerOutput</span><span class="p">,</span> <span class="n">PoolerOutput</span><span class="p">]]]:</span>
  <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">collective_rpc</span><span class="p">(</span><span class="s">"execute_model"</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">execute_model_req</span><span class="p">,</span> <span class="p">))</span>
  <span class="k">return</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<p>The implementation of <code class="language-plaintext highlighter-rouge">collective_rpc</code> is determined by the implementation
of the <code class="language-plaintext highlighter-rouge">_run_workers</code> method per-executor; other executor methods follow a
similar pattern of utilizing collective communication to perform a set of
actions across their subsidiary workers. To date, there exist two executor
implementations: one based on Python’s multiprocessing support (workers are
Python processes; only a single node is supported), and another based on a
distributed Ray cluster (workers are Ray workers; multiple nodes are
supported). An executor, thus, can be understood simply as an abstraction that
defines simple APIs over a collection of workers; each worker handles
implementation details of the executor’s methods, and the executor collects a
response to return to the LLM engine.</p>

<p>Since workers perform model-related operations, they are naturally (hardware)
device-specific. <code class="language-plaintext highlighter-rouge">WorkerBase</code> defines the interface a worker must implement,
with one worker implementation per CPU/GPU/other accelerators<sup id="fnref:7"><a href="#fn:7" class="footnote" rel="footnote" role="doc-noteref">7</a></sup>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">init_device</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
  <span class="s">"""Initialize device state, such as loading the model or other on-device
  memory allocations."""</span>
  <span class="p">...</span>

<span class="k">def</span> <span class="nf">initialize_cache</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_gpu_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_cpu_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
  <span class="s">"""Initialize the KV cache with the given size in blocks."""</span>
  <span class="p">...</span>

<span class="k">def</span> <span class="nf">get_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">:</span>
  <span class="p">...</span>

<span class="k">def</span> <span class="nf">load_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
  <span class="s">"""Load model onto target device."""</span>
  <span class="p">...</span>

<span class="k">def</span> <span class="nf">execute_model</span><span class="p">(</span>
  <span class="bp">self</span><span class="p">,</span>
  <span class="n">execute_model_req</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ExecuteModelRequest</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">SamplerOutput</span><span class="p">]]:</span>
  <span class="p">...</span>

<span class="k">def</span> <span class="nf">determine_num_available_blocks</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
  <span class="s">"""Determine the number of available blocks for the GPU KV cache and
  swappable CPU KV cache.

  The implementation may run profiling or other heuristics to determine
  the size of caches.

  Returns a Tuple[num_gpu_blocks, num_cpu_blocks], where num_gpu_blocks
  are blocks that are "active" on the device and can be appended to.
  num_cpu_blocks refers to "swapped" blocks in CPU memory and cannot be
  appended to.
  """</span>
  <span class="p">...</span>

<span class="k">def</span> <span class="nf">add_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lora_request</span><span class="p">:</span> <span class="n">LoRARequest</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
  <span class="p">...</span>

<span class="k">def</span> <span class="nf">remove_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lora_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
  <span class="p">...</span>

<span class="k">def</span> <span class="nf">pin_lora</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lora_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
  <span class="p">...</span>

<span class="k">def</span> <span class="nf">list_loras</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Set</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
  <span class="p">...</span>
</code></pre></div></div>
<p>Within the <code class="language-plaintext highlighter-rouge">worker</code> subdirectory, you’ll see various device-specific worker
implementations; to date, this includes CPU, HPU, Neuron, Openvino, TPU,
and XPU (sadly, no Metal shader support yet).</p>

<p>In summary:</p>
<ul>
  <li>An <strong>executor</strong> (either multiprocess or Ray) orchestrates collective communication amongst its workers</li>
  <li>A <strong>worker</strong> has a device-specific implementation that runs in its encapsulated process or Ray worker</li>
</ul>

<p>We now turn to the implementation of the model logic that each worker executes;
this is encapsulated in the model runner.</p>

<h2 id="model-runner">Model Runner</h2>

<p>The model runner is not technically a fully separate object: a runner for a
particular device has a 1:1 mapping with a worker implementation for that
device. Instead, the class forms a logical encapsulation of model-related
methods that operate on the worker’s device. In particular, while the worker
implementation manages the KV cache and other collective communication
primitives for its device, the model runner focuses solely on the execution of
the model forward pass. Its interface is simple:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ModelRunnerBase</span><span class="p">(</span><span class="n">ABC</span><span class="p">,</span> <span class="n">Generic</span><span class="p">[</span><span class="n">T</span><span class="p">]):</span>
  <span class="o">@</span><span class="n">abstractmethod</span>
  <span class="k">def</span> <span class="nf">make_model_input_from_broadcasted_tensor_dict</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">tensor_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="s">"""Make an instance of a ModelRunnerInputBase from the broadcasted tensor
    dict."""</span>
    <span class="p">...</span>

  <span class="o">@</span><span class="n">abstractmethod</span>
  <span class="k">def</span> <span class="nf">prepare_model_input</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">seq_group_metadata_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">SequenceGroupMetadata</span><span class="p">],</span>
    <span class="n">virtual_engine</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">finished_requests_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
    <span class="s">"""Prepares the inputs to ModelRunnerBase.execute_model from an execution
    request. This method may move data to the worker's local device. It is not
    allowed to communicate with other workers or devices."""</span>
    <span class="p">...</span>

  <span class="o">@</span><span class="n">abstractmethod</span>
  <span class="k">def</span> <span class="nf">get_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="p">...</span>

  <span class="o">@</span><span class="n">abstractmethod</span>
  <span class="k">def</span> <span class="nf">execute_model</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model_input</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span>
    <span class="n">kv_caches</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">intermediate_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">IntermediateTensors</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">num_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">SamplerOutput</span><span class="p">]]:</span>
    <span class="p">...</span>
</code></pre></div></div>

<p>Note that each model runner has its own device input class (this is the <code class="language-plaintext highlighter-rouge">T</code>
generic type above); this representation is constructed from the
<code class="language-plaintext highlighter-rouge">ExecuteModelRequest</code>, and cast to device-specific tensors (or other
representations entirely) that can be forwarded through the <code class="language-plaintext highlighter-rouge">nn.Model</code>
representing the worker’s shard of the (potentially distributed) LLM.</p>

<p>To build further intuition, we replicate the input preparation and
model execution implementation for distributed workers below (note
that the device-specific implementations are in <code class="language-plaintext highlighter-rouge">&lt;device&gt;_model_runner.py</code>
for the interested reader):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">prepare_input</span><span class="p">(</span>
  <span class="bp">self</span><span class="p">,</span>
  <span class="n">execute_model_req</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ExecuteModelRequest</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">BroadcastableModelInput</span><span class="p">,</span> <span class="n">WorkerInput</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span>
        <span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
  <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">is_driver_worker</span><span class="p">:</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">_get_driver_input_and_broadcast</span><span class="p">(</span><span class="n">execute_model_req</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">_get_worker_input_from_broadcast</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">execute_model</span><span class="p">(</span>
  <span class="bp">self</span><span class="p">,</span>
  <span class="n">execute_model_req</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ExecuteModelRequest</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">SamplerOutput</span><span class="p">]]:</span>
  <span class="c1"># Some portions elided for simplicity:
</span>  <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">prepare_input</span><span class="p">(</span><span class="n">execute_model_req</span><span class="p">)</span>
  <span class="n">model_input</span><span class="p">,</span> <span class="n">worker_input</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="n">inputs</span>

  <span class="c1"># Process an execution request; e.g., issue cache operations
</span>  <span class="c1"># via the cache engine:
</span>  <span class="bp">self</span><span class="p">.</span><span class="n">execute_worker</span><span class="p">(</span><span class="n">worker_input</span><span class="p">)</span>

  <span class="c1"># Pipeline parallelism support:
</span>  <span class="n">intermediate_tensors</span> <span class="o">=</span> <span class="bp">None</span>
  <span class="n">orig_model_execute_time</span> <span class="o">=</span> <span class="mf">0.0</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">get_pp_group</span><span class="p">().</span><span class="n">is_first_rank</span><span class="p">:</span>
    <span class="n">intermediate_tensors</span> <span class="o">=</span> <span class="n">IntermediateTensors</span><span class="p">(</span>
      <span class="n">get_pp_group</span><span class="p">().</span><span class="n">recv_tensor_dict</span><span class="p">(</span>
          <span class="n">all_gather_group</span><span class="o">=</span><span class="n">get_tp_group</span><span class="p">()))</span>

  <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model_runner</span><span class="p">.</span><span class="n">execute_model</span><span class="p">(</span>
      <span class="n">model_input</span><span class="o">=</span><span class="n">model_input</span><span class="p">,</span>
      <span class="n">kv_caches</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">kv_cache</span><span class="p">[</span><span class="n">worker_input</span><span class="p">.</span><span class="n">virtual_engine</span><span class="p">]</span>
      <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">kv_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span>
      <span class="n">intermediate_tensors</span><span class="o">=</span><span class="n">intermediate_tensors</span><span class="p">,</span>
      <span class="n">num_steps</span><span class="o">=</span><span class="n">num_steps</span><span class="p">,</span>
      <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
  <span class="p">)</span>

  <span class="c1"># Pipeline parallelism support:
</span>  <span class="k">if</span> <span class="ow">not</span> <span class="n">get_pp_group</span><span class="p">().</span><span class="n">is_last_rank</span><span class="p">:</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">IntermediateTensors</span><span class="p">)</span>
    <span class="n">get_pp_group</span><span class="p">().</span><span class="n">send_tensor_dict</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="n">tensors</span><span class="p">,</span>
                                    <span class="n">all_gather_group</span><span class="o">=</span><span class="n">get_tp_group</span><span class="p">())</span>
    <span class="k">return</span> <span class="p">[</span><span class="bp">None</span><span class="p">]</span>

  <span class="c1"># Output is List[SamplerOutput]
</span>  <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div>

<p>The output of each model runner (= each worker)’s execution of <code class="language-plaintext highlighter-rouge">execute_model</code>
is pooled across workers in the executor implementation, and returned to the
engine (either synchronously or asynchronously, depending on the type of
engine). The engine then applies any necessary postprocessing and returns to
the user, completing a call to <code class="language-plaintext highlighter-rouge">step</code>.</p>

<h2 id="summary">Summary</h2>

<p>In this post, we’ve touched on the core components that form a vLLM inference
pass. While we haven’t spent much time on the optimizations that make vLLM so
powerful (<em>e.g.</em> the implementation of paged attention, speculative decoding,
flash attention kernels, etc.), hopefully understanding the inference pipeline
end-to-end sheds light on where to dive deeper (for example, the scheduler
<em>via</em> the block allocator and the worker <em>via</em> the cache manager both have a
role to play in managing KV cache blocks; the scheduler manages continuous
batching with tradeoffs between prefill and decode phases, and prefix caching
is handled by the block allocator, which is managed by the scheduler).</p>

<h2 id="notes">Notes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>Hence the name: “virtual”LLM, akin to (paged) virtual memory in an operating system. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>I’m sure I missed out on a lot. Please feel free to drop me a line and correct me. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Virtual engines are a core component of the <a href="https://github.com/vllm-project/vllm/issues/4461">vLLM implementation of pipeline parallelism</a>. Concretely, pipeline parallelism shards a model across multiple devices such that each node hosts a subset of the model’s layers; this is differentiated from tensor parallelism in which model layers themselves (and therefore model intermediate tensors) are sharded along batch (data parallelism) or non-batch dimensions. To achieve proper pipelining, we require multiple independent “streams” that are data independent of one another and can occupy accelerator time when other streams are being executed on other devices—this leads to the concept of a virtual engine in vLLM, where each virtual engine manages one such stream of execution. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>The processed sequence group is precisely added to the scheduler (recall, there exists one scheduler per virtual engine = stream of execution) that has the fewest unfinished sequences, to attempt to load-balance across streams. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>A typical language model inference call can be split into two phases: (typically compute-bound) “prefill”, in which the key-value cache of the prompt is filled by a parallelizable computation of (causal) attention across prompt tokens, and (typically memory-bound) “decode”, in which new tokens are autoregressively generated (potentially with the help of techniques such as speculative decoding). <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p>There is more subtelty when “chunked prefill” is enabled, which chunks and places prefill and decode requests in the same batch to improve GPU utilization. More on that in a later post. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7">
      <p>Most of the worker interface is shared with its parent executor interface. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

    </div>
    <div id='bibliography'>
        <div class='wrap'>
            <ol class="bibliography"></ol>
        </div>
    </div>
</div>
</body>
</html>
