<html>
<head>
    <title>Manan Shah</title>
    <meta charset='UTF-8'>
    <meta content='width=device-width, initial-scale=1' name='viewport'/>

    <meta name='description' content='Manan Shah is a senior at Stanford University.'>
    <!-- A decent browser will parse this fine:
         https://webmasters.stackexchange.com/questions/92744. -->
    <meta name='keywords' content='
        machine learning,
        statistical machine learning,
        bayesian inference,
        statistics,
        computational statistics,
        linear algebra,
        numerical linear algebra,
        statistical software,
        deep learning,
        computer science
    '>
    <meta name='author' content='Manan Shah'>

    <link rel='shortcut icon' href='/favicon.png?v=e' />
    <link href='/css/blog.css' rel='stylesheet'/>

    <script type='text/x-mathjax-config'>
MathJax.Hub.Config({
  jax: ['input/TeX', 'output/HTML-CSS'],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    extensions: ['color.js']
  },
  messageStyle: 'none',
  'HTML-CSS': { preferredFont: 'TeX', availableFonts: ['STIX','TeX'] }
});
</script>

<script src='//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML' type='text/javascript'></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
</head>
<body>
    <div class='nav'>
    <ul class='wrap'>
        <li><a href='/'>Home</a></li>
        <li><a href='/blog'>Blog</a></li>
        <li><a href='/feed.xml'>RSS</a></li>
    </ul>
</div>
    <div id='blog' class='wrap'>
        <div id='intro'>
            <div class='quote'>
                <p>I suppose if we couldn’t laugh at things that don’t make sense, we couldn’t react to a lot of life.</p>
                <a href='https://en.wikiquote.org/wiki/Calvin_and_Hobbes' target='_blank'>Bill Watterson</a>
            </div>
        </div>
        <div id='posts' class='section'>
            
                <div class='post-row'>
                    <p class='post-title'>
                        <a href="/blog/2020/09/06/undirected/">
                            
                            Learning Undirected Graphical Models
                            
                        </a>
                    </p>
                    <p class='post-date'>
                        06 September 2020
                    </p>
                </div>
                <p class='post-subtitle'>
                    
                    Undirected graphical models formed a large part of the initial push for machine intelligence, and remain relevant today. Here, I motivate and derive Monte Carlo-based learning algorithms for such models.
                    
                </p>
                <span class='hidden'>1</span>
            
                <div class='post-row'>
                    <p class='post-title'>
                        <a href="/blog/2020/08/16/fcnn/">
                            
                            Fully Convolutional Networks
                            
                        </a>
                    </p>
                    <p class='post-date'>
                        16 August 2020
                    </p>
                </div>
                <p class='post-subtitle'>
                    
                    I discuss some fundamental ideas behind fully convolutional networks, including the transformation of fully connected layers to convolutional layers and upsampling via transposed convolutions ("deconvolutions").
                    
                </p>
                <span class='hidden'>2</span>
            
                <div class='post-row'>
                    <p class='post-title'>
                        <a href="/blog/2020/08/15/backprop/">
                            
                            Generalized Backpropagation
                            
                        </a>
                    </p>
                    <p class='post-date'>
                        15 August 2020
                    </p>
                </div>
                <p class='post-subtitle'>
                    
                    I motivate and derive the generalized backpropagation algorithm for arbitrarily structured networks.
                    
                </p>
                <span class='hidden'>3</span>
            
                <div class='post-row'>
                    <p class='post-title'>
                        <a href="/blog/2020/07/06/convnets/">
                            
                            Learning Convolutional Networks
                            
                        </a>
                    </p>
                    <p class='post-date'>
                        06 July 2020
                    </p>
                </div>
                <p class='post-subtitle'>
                    
                    I motivate and derive the backpropagation learning algorithm for convolutional networks.
                    
                </p>
                <span class='hidden'>4</span>
            
                <div class='post-row'>
                    <p class='post-title'>
                        <a href="/blog/2020/07/05/ffnn/">
                            
                            Learning Feedforward Networks
                            
                        </a>
                    </p>
                    <p class='post-date'>
                        05 July 2020
                    </p>
                </div>
                <p class='post-subtitle'>
                    
                    I motivate and derive the backpropagation learning algorithm for feedforward networks.
                    
                </p>
                <span class='hidden'>5</span>
            
                <div class='post-row'>
                    <p class='post-title'>
                        <a href="/blog/2020/07/01/trusting-trust/">
                            
                            A Discussion of Ken Thompson's "Reflections on Trusting Trust"
                            
                        </a>
                    </p>
                    <p class='post-date'>
                        01 July 2020
                    </p>
                </div>
                <p class='post-subtitle'>
                    
                    Ken Thompson's Turing award lecture "Reflections on Trusting Trust" took me a while to grasp, but proved immensely rewarding to understand. Here, I discuss the exploit presented in an approachable manner.
                    
                </p>
                <span class='hidden'>6</span>
            
                <div class='post-row'>
                    <p class='post-title'>
                        <a href="/blog/2020/06/25/prob-lr/">
                            
                            A (Formal) Probabilistic Interpretation of Linear Regression
                            
                        </a>
                    </p>
                    <p class='post-date'>
                        25 June 2020
                    </p>
                </div>
                <p class='post-subtitle'>
                    
                    Linear regression is a prolific and natural algorithm often justified probabilistically by assuming that the error in the relationship between target and input variables is Gaussian. Here, I provide a formal proof of this justification.
                    
                </p>
                <span class='hidden'>7</span>
            
                <div class='post-row'>
                    <p class='post-title'>
                        <a href="/blog/2020/06/23/lvm/">
                            
                            Latent Variable Models
                            
                        </a>
                    </p>
                    <p class='post-date'>
                        23 June 2020
                    </p>
                </div>
                <p class='post-subtitle'>
                    
                    Directed latent variable models provide a powerful way to represent complex distributions by combining simple ones. However, they often have intractable log-likelihoods, yielding complicated learning algorithms. In this post, I hope to build intuition for these concepts.
                    
                </p>
                <span class='hidden'>8</span>
            
        </div>
    </div>
</body>
</html>
