<html>
<head>
    <title>Latent Variable Models</title>
    <meta charset='UTF-8'>
    <meta content='width=device-width, initial-scale=1' name='viewport'/>

    <meta name='description' content='Manan Shah is a senior at Stanford University.'>
    <meta name='keywords' content='machine-intelligence'>
    <meta name='author' content='Manan Shah'>

    <link href='/css/callout.css' rel='stylesheet'/>
    <link href='/css/blog.css' rel='stylesheet'/>
    <link href='/css/trac.css' rel='stylesheet'/>
    <link href='/css/markdown.css' rel='stylesheet'/>

    <script type='text/x-mathjax-config'>
MathJax.Hub.Config({
  jax: ['input/TeX', 'output/HTML-CSS'],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    extensions: ['color.js']
  },
  messageStyle: 'none',
  'HTML-CSS': { preferredFont: 'TeX', availableFonts: ['STIX','TeX'] }
});
</script>

<script src='//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML' type='text/javascript'></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

    <script src="/assets/katex.min.js"></script>
    <script src="/assets/pseudocode.min.js" type="text/javascript"></script>
    <link rel="stylesheet" href="/assets/pseudocode.min.css" type="text/css">

</head>
<body>
<div class='content'>
    <div class='nav'>
    <ul class='wrap'>
        <li><a href='/'>Home</a></li>
        <li><a href='/blog'>Blog</a></li>
        <li><a href='/feed.xml'>RSS</a></li>
    </ul>
</div>
    <div class='front-matter'>
        <div class='wrap'>
            <h1>Latent Variable Models</h1>
            <h4>Directed latent variable models provide a powerful way to represent complex distributions by combining simple ones. However, they often have intractable log-likelihoods, yielding complicated learning algorithms. In this post, I hope to build intuition for these concepts.</h4>
            <div class='bylines'>
                <div class='byline'>
                    <h3>Published&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Categories</h3>
                    <p>23 June 2020&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;machine-intelligence</p>
                </div>
            </div>
            <div class='clear'></div>
        </div>
    </div>
    <div class='wrap article'>
        <p>It’s taken me a while to fully grasp directed latent variable models, and I hope
to use this post as a way to collect my thoughts and provide a concise
examination of latent variable representation, learning, and inference.
Significant portions of this material are inspired by Stanford’s <a href="cs228.stanford.edu">CS
228</a> and <a href="cs236.stanford.edu">CS 236</a> courses, taught by Dr.
Stefano Ermon.</p>

<h2 id="introduction">Introduction</h2>

<p>Many modeling problems are framed in a supervised setting, in which one is
provided a dataset $X$ along with outcomes $Y$ with the task of predicting
outcomes for new, unseen samples drawn from the same distribution as $X$.
Discriminative models learn the conditional distribution $p(Y \mid X)$ directly,
and therefore directly predict outcomes given new samples $X$. On the other
hand, generative models specify or learn both $p(Y)$ and $p(X \mid Y)$, and
compute $p(Y \mid X)$ via Bayes’ rule. Both models are powerful in their own
respects: while discriminative models tend to be more expressive as they are
only required to learn the conditional, generative models allow for sampling
new data from $p(X \mid Y)$ and performing inference with some variables $X_i$
are unobserved by marginalizing over the unseen variables.</p>

<p>In an unsupervised setting, in which one is provided a dataset $X$ without
associated outcomes, discriminative modeling assumptions are no longer
meaningful. However, generative models remain powerful: instead of specifying
distributions $p(Y)$ and $p(X \mid Y)$, we now specify distributions $p(Z)$ and
$p(X \mid Z)$ for latent variables $Z$. Intuitively, these variables $Z$
represent unobserved factors of variation that contribute to diversity in $X$;
for example, hair color and eye color in a dataset of faces or pencil stroke
width and shape in a dataset of handwritten images. Learning such latent
representations is both immensely powerful and incredibly challenging: while
effective latent variables can improve modeling and understanding of the
dataset, the unobserved nature of these variables implies that maximum
likelihood cannot be directly applied as in supervised models.</p>

<p>The task of latent variable models (LVMs) is to explicity model latent
variables as defined by the Bayes net $Z \to X$, with $Z$ unobserved and $X$
observed. They therefore learn the joint distribution $p(X, Z; \theta)$ for
parameters $\theta$. The remainder of this post will provide intuition for,
and derivations of, learning algorithms for shallow and deep LVMs.</p>

<h2 id="shallow-latent-variable-models">Shallow Latent Variable Models</h2>

<p>We begin our discussion of latent variable models with shallow LVMs, models that
consist of a simple relationship between $Z$ and $X$. In particular, these
models specify distributions $p(Z)$ and $p(X \mid Z)$ such that the computation
of $p(Z \mid X)$ is tractable. One common example is the Gaussian mixture model,
which specifies $z \sim \text{Categorical}(1 \dots k)$ and $p(x \mid z = k) =
\mathcal{N}(x \mid \mu_k, \sigma_k)$.</p>

<p>To learn such a model via maximum likelihood, we hope to solve the optimization
problem</p>

<script type="math/tex; mode=display">\text{argmax}_\theta \prod_{x \in \mathcal{D}}  p(x; \theta)
= \text{argmax}_\theta \prod_{x \in \mathcal{D}} \sum_{z} p(x, z; \theta)</script>

<p>Our log-likelihood function is therefore</p>

<script type="math/tex; mode=display">\ell(\theta; \mathcal{D}) = \sum_{x \in \mathcal{D}} \log \sum_z p(x, z; \theta)</script>

<p>which does not admit a decomposition conducive to optimization due to the
summation within the logarithm.</p>

<h3 id="a-first-attempt-sampling">A First Attempt: Sampling</h3>

<p>One potential solution to the issues posed by the marginalization over $z$ in the
likelihood function is to perform a <a href="http://statweb.stanford.edu/~susan/courses/s208/node14.html">Monte Carlo
estimate</a> of the
inner sum by sampling $z$ at random and approximating the inner sum with a
sample average:</p>

<script type="math/tex; mode=display">\sum_z p(x, z; \theta) = |\mathcal{Z}| \sum_z \frac{1}{|\mathcal{Z}|} p(x, z;
\theta)  = |\mathcal{Z}| \mathbf{E}_{z \sim \text{Uniform}(\mathcal{Z})} p(x, z;
\theta)</script>

<p>While this works in theory, in practice such estimates tend to perform poorly
as the search space increases exponentially and the majority of randomly
selected $z$s yield a small joint probability. A second, more intricate, attempt
via <a href="https://en.wikipedia.org/wiki/Importance_sampling">importance sampling</a>
with proposal distribution $q(z \mid x)$ yields</p>

<script type="math/tex; mode=display">\begin{align}
\sum_z p(x, z; \theta) = \sum_z q(z \mid x) \frac{p(x, z; \theta)}{q(z \mid x)} = \mathbf{E}_{z \sim q(z \mid x)} \left[ \frac{p(x, z; \theta)}{q(z \mid x)} \right]
\end{align}</script>

<p>where we again approximate the expectation with a sample average, this time from
the proposal distribution $q(z \mid x)$. Doing so alleviates the issue of few
‘‘hits’’ with uniform random sampling in the naive Monte Carlo estimate, given
an appropriate choice of $q(z \mid x)$.</p>

<p>But what should our proposal distribution be? Ideally, we’d like to be sampling
$z \sim p(z \mid x; \theta)$ to choose likely values of the latent variables,
and so a reasonable choice would be $q(z \mid x) = p (z \mid x; \theta)$. Since
we’re working with shallow LVMs, we can safely assume this is a simple
distribution we can sample from (we can compute it with Bayes’ theorem). <sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup></p>

<p>However, while this approach allows us to decompose the joint distribution $p(x,
z; \theta)$, writing the log-likelihood function yields</p>

<script type="math/tex; mode=display">\ell(\theta) = \sum_{x \in \mathcal{D}} \log \mathbf{E}_{z \sim q(z \mid x)} \left[ \frac{p(x, z; \theta)}{q(z \mid x)} \right]</script>

<p>which presents us with another problem: it is difficult to optimize the logarithm
of a sum.</p>

<h3 id="acheiving-tractability-a-lower-bound-on-elltheta">Acheiving Tractability: A Lower Bound on $\ell(\theta)$</h3>

<p>In order to transform the logarithm of a sum into a sum of logarithms in the
log-likelihood function, we apply <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality">Jensen’s
inequality</a>. Doing so
provides us a lower bound on $\ell(\theta)$:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\ell(\theta) &= \sum_{x \in \mathcal{D}} \log \mathbf{E}_{z \sim q(z \mid x)} \left[ \frac{p(x, z; \theta)}{q(z \mid x)} \right] \\
             &\geq \sum_{x \in \mathcal{D}} \mathbf{E}_{z \sim q(z \mid x)} \log \left[ \frac{p(x, z; \theta)}{q(z \mid x)} \right] \label{jensen} \tag{1} \\
             &=  \sum_{x \in \mathcal{D}} \sum_z  q(z \mid x) \log \frac{p(x, z; \theta)}{q(z \mid x)} \label{a} \tag{2} \\
             &=  \sum_{x \in \mathcal{D}} \sum_z  q(z \mid x) \log p(x, z; \theta) - \sum_{x \in \mathcal{D}} \sum_z q(z \mid x) \log q(z \mid x) \label{elbo} \tag{3} \\
\end{align} %]]></script>

<p>where Equation \ref{jensen} is by Jensen. We have therefore arrived at a lower
bound for the likelihood $\ell(\theta)$ that’s optimizable! In fact, the lower
bound in Equation \ref{elbo} is so important that we’ll give it a special name: the
<em>evidence lower bound (ELBO)</em>.</p>

<p>Note that our derivation of the ELBO is independent of a choice of $q(z \mid
x)$. However, as it turns out, our intuitive choice of $q(z \mid x) = p(z \mid
x; \theta)$ has a beautiful property: it makes the bound tight! For proof,
substituting this distribution in Equation \ref{a} yields</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\sum_{x \in \mathcal{D}} \sum_z  q(z \mid x) \log \frac{p(x, z; \theta)}{q(z \mid x)} &= \sum_{x \in \mathcal{D}} \sum_z  p(z \mid x) \log \frac{p(x, z; \theta)}{p(z \mid x)} \\
&= \sum_{x \in \mathcal{D}} \sum_z  p(z \mid x) \log \frac{p(z \mid x) p(x)}{p(z \mid x)} \\
&= \sum_{x \in \mathcal{D}} \sum_z  p(z \mid x) \log p (x) \\
&= \ell(\theta)
\end{align} %]]></script>

<p>as desired. As a result, choosing $q(z \mid x) = p(z \mid x; \theta)$ guarantees
that optimizing the ELBO always increases the likelihood.</p>

<h3 id="expectationmaximization">Expectation—Maximization</h3>

<p>So far, we’ve built intuition for maximizing the LVM log-likelihood function by
drawing insights from importance sampling and subsequently obtaining a tractable
lower bound on the log-likelihood function (the ELBO). With proposal
distribution $q(z \mid x) = p(z \mid x; \theta)$ tractable for shallow LVMs, we 
are guaranteed that the ELBO is tight.</p>

<p>The expectation—maximization algorithm builds upon these ideas, iteratively
optimizing the ELBO over $q$ in the expectation step and the model parameters
$\theta$ in the maximization step. Since the ELBO is tight in the expectation step,
optimization over $\theta$ in the maximization step is guaranteed to increase
the log-likelihood, ensuring that each step of the algorithm makes progress. In
particular, the algorithm proceeds as follows:</p>
<ul>
  <li>Initialize $\theta_0$, either at random or with a good first guess</li>
  <li>Repeat until convergence:</li>
</ul>

<script type="math/tex; mode=display">\theta_{t+1} = \text{argmax}_\theta \sum_{x \in \mathcal{D}} \mathbf{E}_{z \sim p(z \mid x; \theta_t)} \log p(x, z; \theta)</script>

<p>which is broken down into the ‘‘E’’ and ‘‘M’’ steps as follows.</p>

<p><strong>E(xpectation) step.</strong> For each $x \in \mathcal{D}$, compute the proposal
distribution $q(z \mid x) = p(z \mid x, \theta_t)$; this is the posterior
probability for all values $z$ can take. A common interpretation is that we
``hallucinate’’ the missing values of the latent variables $z$ by computing the
distribution over $z$ using our current parameters, $\theta_t$. Note that this
computation requires iterating over all values of $z$ in the discrete case and
integrating in the continuous case, and is therefore only tractable in the
shallow LVM case.</p>

<p><strong>M(aximization) step.</strong> Compute $\theta_{t+1}$ given the posterior computed in
the E step. This requires computing and updating along the gradient;
however, as the logarithm is within the sum, doing so is tractable.</p>

<p>An illustrative example of expectation—maximization for Gaussian mixture models
is located <a href="https://people.csail.mit.edu/rameshvs/content/gmm-em.pdf">here</a>.</p>

<h2 id="deep-latent-variable-models">Deep Latent Variable Models</h2>

<p>We continue our discussion of latent variable models with deep LVMs, models that
consist of a more complicated relationship between $Z$ and $X$. In particular,
we remove the assumption that the $p(Z)$ and $p(X \mid Z)$ are chosen so that
$p(Z \mid X)$ is tractable. While doing so allows for heightened expressivity,
it also invalidates the tractability of $p(z \mid x; \theta)$, a requirement
for the tightness of the ELBO in expectation—maximization.</p>

<p>One common example of a deep LVM is the variational autoencoder (VAE), which
extends the Gaussian mixture model to a mixture of an infinite number of
Gaussian distributions. VAEs are specified as $z \sim \mathcal{N}(0, I)$, $p(x
\mid z) = \mathcal{N}(\mu_\theta(z), \Sigma_\theta(z))$, and $q(z \mid x;
\lambda) = \mathcal{N}(\mu_\lambda(x), \sigma_\lambda(x))$. The necessity of
specifying a proposal distribution $q$ will become evident as we build
intuition for learning deep LVMs.</p>

<p>The log-likelihood function and learning problem for deep LVMs are the same
as those of shallow LVMs.</p>

<h3 id="revisiting-the-elbo">Revisiting the ELBO</h3>

<p>Since the posterior distribution $p(z \mid x; \theta)$ is no longer guaranteed
to be tractable, we can no longer tractably compute the expectations with
respect to the posterior in the E-step of expectation—maximization. We’ll
therefore need a new learning algorithm for deep LVMs; to derive one, let’s
begin by revisiting the evidence lower bound (Equation \ref{elbo}).</p>

<p>Recall that the ELBO is a lower bound to the log-likelihood for all choices of
proposal distribution $q(z)$. To quantify how poor the bound is for an
arbitrary choice of $q(z)$, we can express the
<a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL-divergence</a>
between $q(z)$ and $p(z \mid x; \theta)$ as</p>

<script type="math/tex; mode=display">D_{KL} (q(z) \| p(z \mid x; \theta)) = -\sum_z q(z) \log p(z, x; \theta) + \log p(x; \theta) - \sum_z q(z) \log q(z) \geq 0</script>

<p>which we rearrange to obtain</p>

<script type="math/tex; mode=display">\ell (\theta) = \log p(x; \theta) = \underbrace{\sum_z q(z) \log p(z, x; \theta) - \sum_z q(z) \log q(z)}_{\text{ELBO}} + D_{KL} (q(z) \| p(z \mid x; \theta))</script>

<p>As expected, setting $q(z) = p(z \mid x; \theta)$ makes the ELBO tight since the
KL-divergence between identical distributions is zero. More importantly, since
$p(z \mid x; \theta)$ is intractable for deep LVMs, this formulation of the ELBO
motivates a <em>variational</em> learning algorithm: can we learn a tractable
distribution $q(z; \phi)$ to closely approximate $p(z \mid x; \theta)$? Doing so
would tighten the ELBO, improving our ability to increase $\ell(\theta)$.</p>

<p>This process is termed variational learning <sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup> as it involves the optimization
of $q(z; \phi)$ in function space. Jointly optimizing over our original
paramters $\theta$ and our variational parameters $\phi$ thus provides a
reasonable way to maximize the ELBO over a dataset.</p>

<h3 id="variational-learning">Variational Learning</h3>

<p>Building upon the intuition derived in the previous section, we can write the
ELBO with variational parameters as</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\mathcal{L}(x; \theta, \phi) &= \sum_z q(z; \phi) \log p(z, x; \theta) - \sum_z q(z; \phi) \log q (z; \phi) \\
                             &= \mathbf{E}_{z \sim q(z; \phi)} [\log p(z, x; \theta) - \log q (z; \phi)]
\end{align} %]]></script>

<p>Our new form of maximum likelihood learning over our dataset is to maximize a
lower bound to $\ell(\theta)$:</p>

<script type="math/tex; mode=display">\ell (\theta) = \sum_{x^{(i)} \in \mathcal{D}} \log p(x^{(i)}; \theta) \geq \sum_{x^{(i)} \in \mathcal{D}} \mathcal{L}(x^{(i)}; \theta, \phi^{(i)})</script>

<p>where we note that each data point $x^{(i)}$ has an associated set of
variational parameters $\phi^{(i)}$ as the true posterior $p(z \mid x^{(i)};
\theta)$ is different for each data point $x^{(i)}$. Doing so can be challenging
for large datasets (where such large numbers of parameters makes optimization
expensive), so we instead choose to learn how to map each $x^{(i)}$ to a good
set of parameters $\phi^{(i)}$ via a function $f_\lambda$.<sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup> Specifically, we work
with $q(z; f_\lambda(x))$ for each $x$; in the literature (and for the remainder
of this post), we write $q(z; f_\lambda(x))$ as $q(z \mid x; \lambda)$. Our ELBO
thus has the form</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\mathcal{L}(x; \theta, \lambda) &= \mathbf{E}_{z \sim q(z \mid x; \lambda)} [\log p(z, x; \theta) - \log q(z \mid x; \lambda)]
\end{align} %]]></script>

<p>We optimize the ELBO with gradient descent, updating both the model parameters
$\theta$ and the variational parameters $\lambda$. Our learning algorithm is therefore</p>
<ul>
  <li>Initialize $\theta^{(0)}, \lambda^{(0)}$</li>
  <li>Randomly sample a data point $x^{(i)}$ from $\mathcal{D}$</li>
  <li>Compute $\nabla_\theta \mathcal{L}(x; \theta, \lambda)$ and $\nabla_\lambda \mathcal{L}(x; \theta, \lambda)$</li>
  <li>Update $\theta, \lambda$ in the gradient direction</li>
  <li>Wash, rinse, repeat</li>
</ul>

<h3 id="computing-variational-gradients">Computing Variational Gradients</h3>

<p>Now that we have a learning algorithm, the final piece of the puzzle is to
compute the gradients of the ELBO with respect to $\theta$ and $\lambda$.</p>

<p>Let’s first examine the gradient with respect to $\theta$. We simply have</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\nabla_\theta \mathcal{L}(x; \theta, \lambda) &= \nabla_\theta \mathbf{E}_{z \sim q(z \mid x; \lambda)} [\log p(z, x; \theta) - \log q(z \mid x; \lambda)] \\
                                              &= \mathbf{E}_{z \sim q(z \mid x; \lambda)} [\nabla_\theta \log p(z, x; \theta)]
\end{align} %]]></script>

<p>which we can approximate with Monte Carlo sampling from $q(z \mid x; \lambda)$.</p>

<p>Let’s next consider the gradient with respect to $\lambda$. We have</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\nabla_\lambda \mathcal{L}(x; \theta, \lambda) &= \nabla_\lambda \mathbf{E}_{z \sim q(z \mid x; \lambda)} [\log p(z, x; \theta) - \log q(z \mid x; \lambda)]
\end{align} %]]></script>

<p>but we can’t simply pass the gradient through the expectation as before since the
expectation is itself parameterized by $\lambda$. We can solve this problem in two
ways: a general technique from reinforcement learning called REINFORCE, and a
more stable (but specific) technique called the reparameterization trick. An
excellent article explaining and comparing the two is
<a href="http://stillbreeze.github.io/REINFORCE-vs-Reparameterization-trick/">here</a>:
REINFORCE yields</p>

<script type="math/tex; mode=display">\nabla_\lambda \mathcal{L}(x; \theta, \lambda) = \mathbf{E}_{z \sim q(z \mid x; \lambda)} [(\log p(z, x; \theta) - \log q(z \mid x; \lambda)) \nabla_\lambda \log q(z \mid x; \lambda)]</script>

<p>while the reparametrization trick varies depending on the choice of $q(z \mid x;
\lambda)$ (and only works for continuous $q$ with specific properties); further
information can be found
<a href="https://gregorygundersen.com/blog/2018/04/29/reparameterization/">here</a>.</p>

<h3 id="interpreting-variational-autoencoders">Interpreting Variational Autoencoders</h3>

<p>Earlier in this section, discussing the VAE model required specification of the
variational proposal distribution $q(z \mid x; \lambda)$; as evident from our
derivation of the variational learning algorithm, specifying the class of
distributions from which $q$ is to be learned is necessary. A common interpretation
is that $q(z \mid x; \lambda)$ acts as an ‘‘encoder’’ to latent representation 
$z$, and $p(x \mid z; \theta)$ acts as a ‘‘decoder’’ to the true data distribution.</p>

<p>It so happens that specifying $p (x \mid z; \theta)$ and $q (z \mid x; \lambda)$
as normal distributions as in <a class="citation" href="#kingma2013auto">(Kingma &amp; Welling, 2013)</a> allows for an analytical
simplification of the ELBO as</p>

<script type="math/tex; mode=display">\mathcal{L}(x; \theta, \lambda) = \underbrace{D_{KL} (q(z \mid x; \lambda) \|
p(z))}_{\text{Analytically compute this}} + \underbrace{\mathbf{E}_{z \sim q(z \mid x;
\lambda)} \log p(x \mid z; \theta)}_{\text{Monte Carlo estimate this}}</script>

<p>This representation also has a nice interpretation: the first term encourages
latent representations to be likely under the prior $p(z)$, and the second term
encourages $x$ to be likely given its latent representation.</p>

<h2 id="summary--further-reading">Summary &amp; Further Reading</h2>

<p>Latent variable models are incredibly useful frameworks that combine simple 
distributions to create more complicated ones. Defined by the Bayes net structure
$Z \to X$, they permit ancestral sampling for efficient generation ($z \sim p(z)$
and $x \sim p(x \mid z; \theta))$, but often have intractable log-likelihoods,
making learning difficult.</p>

<p>Both shallow and deep LVMs therefore optimize a lower bound to the log
likelihood, called the ELBO. While shallow LVMs make the ELBO tight by
explicitly computing $q(z \mid x; \theta) = p(z \mid x; \theta)$, this
computation is intractable for deep LVMs, which use variational learning to
learn a distribution $q(z \mid x; \lambda)$ that best approximates $p(z \mid x;
\theta)$. Jointly learning the model $\theta$ and the amortized inference component
$\lambda$ helps deep LVMs acheive tractability for ELBO optimization.</p>

<p>Many other types of latent variable models which perform learning without
worrying about the ELBO weren’t covered in this post. For the interested
reader, normalizing flow models (using invertible transformations) and GANs
(likelihood-free models) are exciting avenues for further reading.</p>

<h2 id="notes">Notes</h2>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>If this worries you, that’s good! We throw away this assumption with deep latent variable models, but doing so makes learning far more complicated… <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>'’Variational’’ is a term borrowed from variational calculus; in our context, it refers to the process of optimizing over functions. <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Learning a single parametric function $f_\lambda : x \to \phi$ that maps each $x$ to a set of variational parameters is called amortized inference, as the process of inferring $z$ given $x$ is amortized over all training examples for sake of tractability. <a href="#fnref:3" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

    </div>
    <div id='bibliography'>
        <div class='wrap'>
            <ol class="bibliography"><li><span id="kingma2013auto">Kingma, D. P., &amp; Welling, M. (2013). Auto-encoding variational bayes. <i>ArXiv Preprint ArXiv:1312.6114</i>.</span></li></ol>
        </div>
    </div>
</div>
</body>
</html>